import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from numpy.linalg import norm 

def loadData():
    with np.load('notMNIST.npz') as data :
        Data, Target = data ['images'], data['labels']
        posClass = 2
        negClass = 9
        dataIndx = (Target==posClass) + (Target==negClass)
        Data = Data[dataIndx]/255.
        Target = Target[dataIndx].reshape(-1, 1)
        Target[Target==posClass] = 1
        Target[Target==negClass] = 0
        np.random.seed(421)
        randIndx = np.arange(len(Data))
        np.random.shuffle(randIndx)
        Data, Target = Data[randIndx], Target[randIndx]
        trainData, trainTarget = Data[:3500], Target[:3500]
        validData, validTarget = Data[3500:3600], Target[3500:3600]
        testData, testTarget = Data[3600:], Target[3600:]
    return trainData, validData, testData, trainTarget, validTarget, testTarget

1.0 Linear Regression

def MSE(W, b, x, y, reg):
    # Your implementation here
    """
    W: weight matrix 
    b: bias 
    x: data matrix 
    y: target vector, ground truth 
    reg: regulator scaler 
    output: MSE loss value  
    """
    error=np.matmul(x,W) + b - y
    firstterm=(norm(error)**2)/error.shape[0]
    secondterm=reg*(norm(W)**2)
    #firstterm = np.mean(error**2)
    #secondterm = reg * (np.sum(np.square(W)))/(2.0)
    #secondterm = reg * (norm(W)**2)/(2.0)
    mse = firstterm + secondterm
    
    return mse

def gradMSE(W, b, x, y, reg):
    # Your implementation here
    """
    W: weight matrix 
    b: bias 
    x: data matrix 
    y: target vector
    reg: regulator scaler 
    output: weight gradient vector, bias gradient value   
    """
    mse = np.matmul(x, W) + b - y
    N = np.shape(x)[0]
    grad_b =  2.0*np.mean(mse)
    grad_W =  2.0*np.matmul(np.transpose(x), mse)/N + 2 * reg * W
    return grad_b, grad_W 

def grad_descent(W, b, trainData,trainTarget,validData,validTargety, alpha, epochs, reg, error_tol=1e-7,lossType="None", plotFreq=100):
    """
    W: weight matrix 
    b: bias 
    trainData: training data
    trainTarget: training target
    validData: validation data
    validTarget: validation target
    alpha: learning rate
    epochs: number of iterations
    reg: regulator scaler 
    error_tol: minimum error, default 1e-7
    lossType: specify loss type, either "MSE" or "corssEntropyLoss", default value "None" 
    plotFreq: save the loss for every plotFreq iteration
    output: W, b, numOfEpochs, trainLoss, valLoss
    """

    numOfEpochs=[]
    trainLoss=[]
    valLoss=[]

    if lossType == "MSE":       
     for i in range(epochs):
      grad_b, grad_W = gradMSE(W,b,trainData,trainTarget,reg)
      update_W = W - alpha * grad_W
      update_b = b - alpha * grad_b
      diff =  norm(update_W-W)
      if i % plotFreq == 0:
        numOfEpochs.append(i)
        trainLoss.append(MSE(update_W,update_b,trainData,trainTarget,reg))
        valLoss.append(MSE(update_W,update_b,validData,validTarget,reg))
      if diff <= error_tol:
        return update_W, update_b, numOfEpochs, trainLoss,valLoss
      else:
        W = update_W
        b = update_b
    return update_W, update_b, numOfEpochs, trainLoss,valLoss
    #end elif

2.0 Logistic Regression

def sigmoid (x):
  return np.divide(np.exp(x), 1 + np.exp(x))

def crossEntropy(W,b,x,y,reg):
  yHat = (1.0)/(1.0+np.exp((-1.0)*(np.dot(x,W)+b)))
  loss = np.mean(-(y*np.log(yHat)+(1.0-y)*np.log(1-yHat)))
  weightDecay = reg*(np.sum(W*W))/(2.0)
  return loss + weightDecay

def gradCE(W,b,x,y,reg):
  yHat = (1.0)/(1.0+np.exp((-1.0)*(np.dot(x,W)+b)))
  grad_W = np.dot(x.T,(yHat-y))/(x.shape[0])+(2.0)*(reg)*(W)
  grad_b = np.mean(yHat-y)
  return grad_b, grad_W